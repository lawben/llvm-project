; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=aarch64-apple-darwin -verify-machineinstrs < %s | FileCheck %s

define <4 x i32> @test_compress_v4i32(<4 x i32> %vec, <4 x i1> %mask) {
; CHECK-LABEL: test_compress_v4i32:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    ushll.4s v1, v1, #0
; CHECK-NEXT:    mov x8, sp
; CHECK-NEXT:    str s0, [sp]
; CHECK-NEXT:    shl.4s v1, v1, #31
; CHECK-NEXT:    cmlt.4s v1, v1, #0
; CHECK-NEXT:    mov.s w9, v1[1]
; CHECK-NEXT:    mov.s w10, v1[2]
; CHECK-NEXT:    fmov w11, s1
; CHECK-NEXT:    bfi x8, x11, #2, #1
; CHECK-NEXT:    and w11, w11, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    add w9, w11, w9
; CHECK-NEXT:    mov x11, sp
; CHECK-NEXT:    st1.s { v0 }[1], [x8]
; CHECK-NEXT:    add w10, w9, w10
; CHECK-NEXT:    orr x9, x11, x9, lsl #2
; CHECK-NEXT:    bfi x11, x10, #2, #2
; CHECK-NEXT:    st1.s { v0 }[2], [x9]
; CHECK-NEXT:    st1.s { v0 }[3], [x11]
; CHECK-NEXT:    ldr q0, [sp], #16
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress.v4i32(<4 x i32> %vec, <4 x i1> %mask)
    ret <4 x i32> %out
}

define <2 x double> @test_compress_v2f64(<2 x double> %vec, <2 x i1> %mask) {
; CHECK-LABEL: test_compress_v2f64:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    ushll.2d v1, v1, #0
; CHECK-NEXT:    mov x8, sp
; CHECK-NEXT:    str d0, [sp]
; CHECK-NEXT:    shl.2d v1, v1, #63
; CHECK-NEXT:    cmlt.2d v1, v1, #0
; CHECK-NEXT:    fmov x9, d1
; CHECK-NEXT:    bfi x8, x9, #3, #1
; CHECK-NEXT:    st1.d { v0 }[1], [x8]
; CHECK-NEXT:    ldr q0, [sp], #16
; CHECK-NEXT:    ret
    %out = call <2 x double> @llvm.masked.compress.v2f64(<2 x double> %vec, <2 x i1> %mask)
    ret <2 x double> %out
}

define <16 x i8> @test_compress_v16i8(<16 x i8> %vec, <16 x i1> %mask) {
; CHECK-LABEL: test_compress_v16i8:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    shl.16b v1, v1, #7
; CHECK-NEXT:    mov x12, sp
; CHECK-NEXT:    mov x8, sp
; CHECK-NEXT:    st1.b { v0 }[0], [x8]
; CHECK-NEXT:    mov x13, sp
; CHECK-NEXT:    cmlt.16b v1, v1, #0
; CHECK-NEXT:    umov.b w9, v1[0]
; CHECK-NEXT:    umov.b w10, v1[1]
; CHECK-NEXT:    umov.b w11, v1[2]
; CHECK-NEXT:    umov.b w14, v1[3]
; CHECK-NEXT:    bfxil x12, x9, #0, #1
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    add w9, w9, w10
; CHECK-NEXT:    umov.b w10, v1[4]
; CHECK-NEXT:    and w11, w11, #0x1
; CHECK-NEXT:    st1.b { v0 }[1], [x12]
; CHECK-NEXT:    orr x12, x8, x9
; CHECK-NEXT:    add w9, w9, w11
; CHECK-NEXT:    umov.b w11, v1[5]
; CHECK-NEXT:    and w14, w14, #0x1
; CHECK-NEXT:    st1.b { v0 }[2], [x12]
; CHECK-NEXT:    add w14, w9, w14
; CHECK-NEXT:    umov.b w12, v1[6]
; CHECK-NEXT:    orr x9, x8, x9
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    st1.b { v0 }[3], [x9]
; CHECK-NEXT:    orr x9, x8, x14
; CHECK-NEXT:    add w10, w14, w10
; CHECK-NEXT:    umov.b w14, v1[7]
; CHECK-NEXT:    st1.b { v0 }[4], [x9]
; CHECK-NEXT:    and w11, w11, #0x1
; CHECK-NEXT:    bfxil x13, x10, #0, #4
; CHECK-NEXT:    mov x9, sp
; CHECK-NEXT:    add w10, w10, w11
; CHECK-NEXT:    umov.b w11, v1[8]
; CHECK-NEXT:    and w12, w12, #0x1
; CHECK-NEXT:    bfxil x9, x10, #0, #4
; CHECK-NEXT:    st1.b { v0 }[5], [x13]
; CHECK-NEXT:    umov.b w13, v1[9]
; CHECK-NEXT:    add w10, w10, w12
; CHECK-NEXT:    mov x12, sp
; CHECK-NEXT:    and w14, w14, #0x1
; CHECK-NEXT:    st1.b { v0 }[6], [x9]
; CHECK-NEXT:    umov.b w9, v1[10]
; CHECK-NEXT:    bfxil x12, x10, #0, #4
; CHECK-NEXT:    add w10, w10, w14
; CHECK-NEXT:    mov x14, sp
; CHECK-NEXT:    and w11, w11, #0x1
; CHECK-NEXT:    bfxil x14, x10, #0, #4
; CHECK-NEXT:    add w10, w10, w11
; CHECK-NEXT:    mov x11, sp
; CHECK-NEXT:    and w13, w13, #0x1
; CHECK-NEXT:    st1.b { v0 }[7], [x12]
; CHECK-NEXT:    mov x12, sp
; CHECK-NEXT:    bfxil x11, x10, #0, #4
; CHECK-NEXT:    add w10, w10, w13
; CHECK-NEXT:    umov.b w13, v1[11]
; CHECK-NEXT:    st1.b { v0 }[8], [x14]
; CHECK-NEXT:    umov.b w14, v1[12]
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    bfxil x12, x10, #0, #4
; CHECK-NEXT:    add w9, w10, w9
; CHECK-NEXT:    mov x10, sp
; CHECK-NEXT:    st1.b { v0 }[9], [x11]
; CHECK-NEXT:    umov.b w11, v1[13]
; CHECK-NEXT:    bfxil x10, x9, #0, #4
; CHECK-NEXT:    st1.b { v0 }[10], [x12]
; CHECK-NEXT:    umov.b w12, v1[14]
; CHECK-NEXT:    and w13, w13, #0x1
; CHECK-NEXT:    and w14, w14, #0x1
; CHECK-NEXT:    add w9, w9, w13
; CHECK-NEXT:    st1.b { v0 }[11], [x10]
; CHECK-NEXT:    mov x10, sp
; CHECK-NEXT:    add w13, w9, w14
; CHECK-NEXT:    mov x14, sp
; CHECK-NEXT:    bfxil x10, x9, #0, #4
; CHECK-NEXT:    and w9, w11, #0x1
; CHECK-NEXT:    mov x11, sp
; CHECK-NEXT:    add w9, w13, w9
; CHECK-NEXT:    and w12, w12, #0x1
; CHECK-NEXT:    bfxil x14, x13, #0, #4
; CHECK-NEXT:    bfxil x11, x9, #0, #4
; CHECK-NEXT:    add w9, w9, w12
; CHECK-NEXT:    st1.b { v0 }[12], [x10]
; CHECK-NEXT:    bfxil x8, x9, #0, #4
; CHECK-NEXT:    st1.b { v0 }[13], [x14]
; CHECK-NEXT:    st1.b { v0 }[14], [x11]
; CHECK-NEXT:    st1.b { v0 }[15], [x8]
; CHECK-NEXT:    ldr q0, [sp], #16
; CHECK-NEXT:    ret
    %out = call <16 x i8> @llvm.masked.compress.v16i8(<16 x i8> %vec, <16 x i1> %mask)
    ret <16 x i8> %out
}

define <8 x i32> @test_compress_large(<8 x i32> %vec, <8 x i1> %mask) {
; CHECK-LABEL: test_compress_large:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    ; kill: def $d2 killed $d2 def $q2
; CHECK-NEXT:    umov.b w8, v2[0]
; CHECK-NEXT:    umov.b w9, v2[1]
; CHECK-NEXT:    mov x12, sp
; CHECK-NEXT:    umov.b w10, v2[2]
; CHECK-NEXT:    umov.b w13, v2[3]
; CHECK-NEXT:    mov x11, sp
; CHECK-NEXT:    umov.b w14, v2[4]
; CHECK-NEXT:    str s0, [sp]
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    and w15, w8, #0x1
; CHECK-NEXT:    bfi x12, x8, #2, #1
; CHECK-NEXT:    and w8, w10, #0x1
; CHECK-NEXT:    add w9, w15, w9
; CHECK-NEXT:    umov.b w10, v2[5]
; CHECK-NEXT:    add w8, w9, w8
; CHECK-NEXT:    orr x15, x11, x9, lsl #2
; CHECK-NEXT:    umov.b w9, v2[6]
; CHECK-NEXT:    st1.s { v0 }[1], [x12]
; CHECK-NEXT:    add x12, x11, w8, uxtw #2
; CHECK-NEXT:    and w13, w13, #0x1
; CHECK-NEXT:    st1.s { v0 }[2], [x15]
; CHECK-NEXT:    add w8, w8, w13
; CHECK-NEXT:    st1.s { v0 }[3], [x12]
; CHECK-NEXT:    and w12, w14, #0x1
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    add w12, w8, w12
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    and x8, x8, #0x7
; CHECK-NEXT:    add w10, w12, w10
; CHECK-NEXT:    and x12, x12, #0x7
; CHECK-NEXT:    str s1, [x11, x8, lsl #2]
; CHECK-NEXT:    add w9, w10, w9
; CHECK-NEXT:    and x10, x10, #0x7
; CHECK-NEXT:    add x12, x11, x12, lsl #2
; CHECK-NEXT:    and x9, x9, #0x7
; CHECK-NEXT:    add x8, x11, x10, lsl #2
; CHECK-NEXT:    add x9, x11, x9, lsl #2
; CHECK-NEXT:    st1.s { v1 }[1], [x12]
; CHECK-NEXT:    st1.s { v1 }[2], [x8]
; CHECK-NEXT:    st1.s { v1 }[3], [x9]
; CHECK-NEXT:    ldp q0, q1, [sp], #32
; CHECK-NEXT:    ret
    %out = call <8 x i32> @llvm.masked.compress.v8i32(<8 x i32> %vec, <8 x i1> %mask)
    ret <8 x i32> %out
}

define <4 x i32> @test_compress_all_const() {
; CHECK:       lCPI4_0:
; CHECK-NEXT:	.long	5
; CHECK-NEXT:	.long	9
; CHECK-LABEL: test_compress_all_const:
; CHECK:       ; %bb.0:
; CHECK-NEXT:  Lloh0:
; CHECK-NEXT:    adrp x8, lCPI4_0@PAGE
; CHECK-NEXT:  Lloh1:
; CHECK-NEXT:    ldr q0, [x8, lCPI4_0@PAGEOFF]
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> <i32 3, i32 5, i32 7, i32 9>,
                                                <4 x i1>   <i1 0,  i1 1,  i1 0,  i1 1>)
    ret <4 x i32> %out
}

define <4 x i32> @test_compress_const_mask(<4 x i32> %vec) {
; CHECK-LABEL: test_compress_const_mask:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    mov.s v0[1], v0[3]
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> %vec, <4 x i1> <i1 1, i1 undef, i1 0, i1 1>)
    ret <4 x i32> %out
}

; We pass a placeholder value for the const_mask* tests to check that they are converted to a no-op by simply copying
; the second vector input register to the return register or doing nothing.
define <4 x i32> @test_compress_const_splat1_mask(<4 x i32> %ignore, <4 x i32> %vec) {
; CHECK-LABEL: test_compress_const_splat1_mask:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    mov.16b v0, v1
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> %vec, <4 x i1> splat (i1 -1))
    ret <4 x i32> %out
}
define <4 x i32> @test_compress_const_splat0_mask(<4 x i32> %ignore, <4 x i32> %vec) {
; CHECK-LABEL: test_compress_const_splat0_mask:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> %vec, <4 x i1> splat (i1 0))
    ret <4 x i32> %out
}
define <4 x i32> @test_compress_undef_mask(<4 x i32> %ignore, <4 x i32> %vec) {
; CHECK-LABEL: test_compress_undef_mask:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> %vec, <4 x i1> undef)
    ret <4 x i32> %out
}

define <4 x i8> @test_compress_small(<4 x i8> %vec, <4 x i1> %mask) {
; CHECK-LABEL: test_compress_small:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    shl.4h v1, v1, #15
; CHECK-NEXT:    add x8, sp, #8
; CHECK-NEXT:    ; kill: def $d0 killed $d0 def $q0
; CHECK-NEXT:    str h0, [sp, #8]
; CHECK-NEXT:    cmlt.4h v1, v1, #0
; CHECK-NEXT:    umov.h w9, v1[0]
; CHECK-NEXT:    umov.h w10, v1[1]
; CHECK-NEXT:    umov.h w11, v1[2]
; CHECK-NEXT:    bfi x8, x9, #1, #1
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    add w9, w9, w10
; CHECK-NEXT:    and w11, w11, #0x1
; CHECK-NEXT:    add x10, sp, #8
; CHECK-NEXT:    add w11, w9, w11
; CHECK-NEXT:    orr x9, x10, x9, lsl #1
; CHECK-NEXT:    st1.h { v0 }[1], [x8]
; CHECK-NEXT:    bfi x10, x11, #1, #2
; CHECK-NEXT:    st1.h { v0 }[2], [x9]
; CHECK-NEXT:    st1.h { v0 }[3], [x10]
; CHECK-NEXT:    ldr d0, [sp, #8]
; CHECK-NEXT:    add sp, sp, #16
; CHECK-NEXT:    ret
    %out = call <4 x i8> @llvm.masked.compress.v4i8(<4 x i8> %vec, <4 x i1> %mask)
    ret <4 x i8> %out
}

define <4 x i4> @test_compress_illegal_element_type(<4 x i4> %vec, <4 x i1> %mask) {
; CHECK-LABEL: test_compress_illegal_element_type:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    shl.4h v1, v1, #15
; CHECK-NEXT:    add x8, sp, #8
; CHECK-NEXT:    ; kill: def $d0 killed $d0 def $q0
; CHECK-NEXT:    str h0, [sp, #8]
; CHECK-NEXT:    cmlt.4h v1, v1, #0
; CHECK-NEXT:    umov.h w9, v1[0]
; CHECK-NEXT:    umov.h w10, v1[1]
; CHECK-NEXT:    umov.h w11, v1[2]
; CHECK-NEXT:    bfi x8, x9, #1, #1
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    add w9, w9, w10
; CHECK-NEXT:    and w11, w11, #0x1
; CHECK-NEXT:    add x10, sp, #8
; CHECK-NEXT:    add w11, w9, w11
; CHECK-NEXT:    orr x9, x10, x9, lsl #1
; CHECK-NEXT:    st1.h { v0 }[1], [x8]
; CHECK-NEXT:    bfi x10, x11, #1, #2
; CHECK-NEXT:    st1.h { v0 }[2], [x9]
; CHECK-NEXT:    st1.h { v0 }[3], [x10]
; CHECK-NEXT:    ldr d0, [sp, #8]
; CHECK-NEXT:    add sp, sp, #16
; CHECK-NEXT:    ret
    %out = call <4 x i4> @llvm.masked.compress.v4i4(<4 x i4> %vec, <4 x i1> %mask)
    ret <4 x i4> %out
}

define <3 x i32> @test_compress_narrow(<3 x i32> %vec, <3 x i1> %mask) {
; CHECK-LABEL: test_compress_narrow:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    fmov s1, w0
; CHECK-NEXT:    mov x11, sp
; CHECK-NEXT:    str s0, [sp]
; CHECK-NEXT:    mov.h v1[1], w1
; CHECK-NEXT:    mov.h v1[2], w2
; CHECK-NEXT:    ushll.4s v1, v1, #0
; CHECK-NEXT:    shl.4s v1, v1, #31
; CHECK-NEXT:    cmlt.4s v1, v1, #0
; CHECK-NEXT:    mov.s w8, v1[1]
; CHECK-NEXT:    mov.s w9, v1[2]
; CHECK-NEXT:    fmov w10, s1
; CHECK-NEXT:    bfi x11, x10, #2, #1
; CHECK-NEXT:    and w10, w10, #0x1
; CHECK-NEXT:    and w8, w8, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    add w8, w10, w8
; CHECK-NEXT:    mov x10, sp
; CHECK-NEXT:    st1.s { v0 }[1], [x11]
; CHECK-NEXT:    add w9, w8, w9
; CHECK-NEXT:    orr x8, x10, x8, lsl #2
; CHECK-NEXT:    bfi x10, x9, #2, #2
; CHECK-NEXT:    st1.s { v0 }[2], [x8]
; CHECK-NEXT:    st1.s { v0 }[3], [x10]
; CHECK-NEXT:    ldr q0, [sp], #16
; CHECK-NEXT:    ret
    %out = call <3 x i32> @llvm.masked.compress.v3i32(<3 x i32> %vec, <3 x i1> %mask)
    ret <3 x i32> %out
}

define <3 x i3> @test_compress_narrow_illegal_element_type(<3 x i3> %vec, <3 x i1> %mask) {
; CHECK-LABEL: test_compress_narrow_illegal_element_type:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    fmov s0, w3
; CHECK-NEXT:    add x10, sp, #8
; CHECK-NEXT:    strh w0, [sp, #8]
; CHECK-NEXT:    mov.h v0[1], w4
; CHECK-NEXT:    mov.h v0[2], w5
; CHECK-NEXT:    shl.4h v0, v0, #15
; CHECK-NEXT:    cmlt.4h v0, v0, #0
; CHECK-NEXT:    umov.h w8, v0[0]
; CHECK-NEXT:    umov.h w9, v0[1]
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    and w11, w8, #0x1
; CHECK-NEXT:    bfi x10, x8, #1, #1
; CHECK-NEXT:    add w8, w11, w9
; CHECK-NEXT:    add x9, sp, #8
; CHECK-NEXT:    orr x8, x9, x8, lsl #1
; CHECK-NEXT:    strh w1, [x10]
; CHECK-NEXT:    strh w2, [x8]
; CHECK-NEXT:    ldr d0, [sp, #8]
; CHECK-NEXT:    umov.h w0, v0[0]
; CHECK-NEXT:    umov.h w1, v0[1]
; CHECK-NEXT:    umov.h w2, v0[2]
; CHECK-NEXT:    add sp, sp, #16
; CHECK-NEXT:    ret
    %out = call <3 x i3> @llvm.masked.compress.v3i3(<3 x i3> %vec, <3 x i1> %mask)
    ret <3 x i3> %out
}

define void @test_compress_store(<4 x i32> %vec, <4 x i1> %mask, ptr %ptr) {
; CHECK-LABEL: test_compress_store:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    ushll.4s v1, v1, #0
; CHECK-NEXT:    str s0, [x0]
; CHECK-NEXT:    shl.4s v1, v1, #31
; CHECK-NEXT:    cmlt.4s v1, v1, #0
; CHECK-NEXT:    mov.s w8, v1[1]
; CHECK-NEXT:    mov.s w9, v1[2]
; CHECK-NEXT:    fmov w10, s1
; CHECK-NEXT:    and w11, w10, #0x1
; CHECK-NEXT:    and x10, x10, #0x1
; CHECK-NEXT:    and w8, w8, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    add x10, x0, x10, lsl #2
; CHECK-NEXT:    add w8, w11, w8
; CHECK-NEXT:    add w9, w8, w9
; CHECK-NEXT:    add x8, x0, w8, uxtw #2
; CHECK-NEXT:    st1.s { v0 }[1], [x10]
; CHECK-NEXT:    and x9, x9, #0x3
; CHECK-NEXT:    add x9, x0, x9, lsl #2
; CHECK-NEXT:    st1.s { v0 }[2], [x8]
; CHECK-NEXT:    st1.s { v0 }[3], [x9]
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> %vec, <4 x i1> %mask)
    store <4 x i32> %out, ptr %ptr
    ret void
}

define <4 x i32> @test_compress_store_with_ret(<4 x i32> %vec, <4 x i1> %mask, ptr %ptr) {
; CHECK-LABEL: test_compress_store_with_ret:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    ushll.4s v1, v1, #0
; CHECK-NEXT:    str s0, [x0]
; CHECK-NEXT:    shl.4s v1, v1, #31
; CHECK-NEXT:    cmlt.4s v1, v1, #0
; CHECK-NEXT:    mov.s w8, v1[1]
; CHECK-NEXT:    mov.s w9, v1[2]
; CHECK-NEXT:    fmov w10, s1
; CHECK-NEXT:    and w11, w10, #0x1
; CHECK-NEXT:    and x10, x10, #0x1
; CHECK-NEXT:    and w8, w8, #0x1
; CHECK-NEXT:    and w9, w9, #0x1
; CHECK-NEXT:    add x10, x0, x10, lsl #2
; CHECK-NEXT:    add w8, w11, w8
; CHECK-NEXT:    add w9, w8, w9
; CHECK-NEXT:    add x8, x0, w8, uxtw #2
; CHECK-NEXT:    st1.s { v0 }[1], [x10]
; CHECK-NEXT:    and x9, x9, #0x3
; CHECK-NEXT:    add x9, x0, x9, lsl #2
; CHECK-NEXT:    st1.s { v0 }[2], [x8]
; CHECK-NEXT:    st1.s { v0 }[3], [x9]
; CHECK-NEXT:    ldr q0, [x0]
; CHECK-NEXT:    ret
    %out = call <4 x i32> @llvm.masked.compress(<4 x i32> %vec, <4 x i1> %mask)
    store <4 x i32> %out, ptr %ptr
    ret <4 x i32> %out
}
